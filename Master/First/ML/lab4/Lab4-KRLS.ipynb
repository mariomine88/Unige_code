{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Kernel Regularized Least Squares\n",
    "\n",
    "In this lab activity we consider the extension of regularized least squares to non-linear problems using kernel functions.\n",
    "\n",
    "A brief summary of the tasks:\n",
    " 1. Generate a simple non-linear data-set\n",
    " 2. Use **linear** RLS to try and learn with such dataset\n",
    " 3. Use a **feature transformation** for learning with non-linear data\n",
    " 4. Implement various kernel functions\n",
    " 5. Implement kernel RLS\n",
    " 6. Generate a more complex non-linear data-set\n",
    " 7. Use kernel RLS for learning on non-linear data, use cross-validation to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy.random import Generator\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial\n",
    "\n",
    "\n",
    "def _check_random_generator(rng: int | Generator | None) -> Generator:\n",
    "    \"\"\"Convert rng into a np.random.Generator instance.\"\"\"\n",
    "    if rng is None:\n",
    "        print(\"âš ï¸  Warning: Set the `rng` parameter to a NumPy random generator\\n\"\n",
    "              \"   (e.g. `np.random.default_rng(42)`) or specify a fixed seed to ensure\\n\"\n",
    "              \"   your results are reproducible. Proceeding with a random seed for now.\")\n",
    "        return np.random.default_rng()\n",
    "    if isinstance(rng, np.random.Generator):\n",
    "        return rng\n",
    "    if isinstance(rng, (int, np.integer)):\n",
    "        return np.random.default_rng(rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_err(Ypred, Ytrue):\n",
    "    return np.mean((Ypred-Ytrue)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Learning with a simple non-linear dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate quadratic data\n",
    "\n",
    "In this lab we are going to use regression datasets where the target `Y` is **not a linear function** of the inputs `X`.\n",
    "\n",
    "As a first example, see the following function to generate quadratic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_data_gen(n, w, sigma_noise, rng=None):\n",
    "    rng = _check_random_generator(rng)\n",
    "\n",
    "    X = rng.uniform(-3, 3, size=(n, 1))\n",
    "    Xsq = X ** 2\n",
    "    noise = rng.normal(0, sigma_noise, size=(n, 1))\n",
    "\n",
    "    # Here we can use scalar multiplication since in dimension 1\n",
    "    Y = Xsq * w + noise\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(100, np.array([3]), 0.5)\n",
    "\n",
    "print(f\"Shape of x: {x_tr.shape}, shape of y: {y_tr.shape}\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_tr, y_tr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear RLS\n",
    "\n",
    "The implementation of linear regularized least-squares is given below in the functions `rls_train` and `rls_predict`. \n",
    "\n",
    "Remember: regularized least-squares has the following form\n",
    "\n",
    "$$(X_{tr}^\\top X_{tr} + \\lambda n I)w = X_{tr}^\\top Y_{tr}$$\n",
    "\n",
    "\n",
    "Tasks:\n",
    " - Use RLS to train a linear regressor for the quadratic data. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rls_train(x, y, reg_par):\n",
    "    cov = x.T @ x + reg_par * x.shape[0] * np.eye(x.shape[1])\n",
    "    rhs = x.T @ y\n",
    "    w = np.linalg.solve(cov, rhs)\n",
    "    return w\n",
    "\n",
    "def rls_predict(x, w):\n",
    "    return x @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(100, np.array([3]), 0.5)\n",
    "\n",
    "# TODO: put your code here\n",
    "w = ...\n",
    "pred_tr = ...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_tr, y_tr, label=\"True\")\n",
    "ax.scatter(x_tr, pred_tr, label=\"Predicted\")\n",
    "ax.legend(loc=\"upper center\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Feature transform\n",
    "\n",
    "There is a simple way to use a linear algorithm for learning non-linear data: transforming the input data in such a way to convert the problem into a linear one.\n",
    "\n",
    "This is a simple fix in some cases, but becomes cumbersome if the datasets are non-linear in a complex way.\n",
    "\n",
    "Here we adopt this approach to train a RLS classifier with the quadratic dataset:\n",
    " 1. Generate the quadratic dataset\n",
    " 2. Transform the data so that it becomes a (n, 2) matrix containing the original input, and a transformed version of itself. Clearly the correct transformation depends on the underlying function (a quadratic function!).\n",
    " 3. Use the RLS algorithm on the new dataset\n",
    " 4. Plot **and comment** on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(100, np.array([3]), 0.5)\n",
    "\n",
    "# TODO: Instead of just the given xs build a feature matrix with the xs and their squares:\n",
    "trsf_x_tr = np.concatenate([...], axis=1)\n",
    "assert trsf_x_tr.shape == (x_tr.shape[0], 2), f\"Shape of x_tr is {x_tr.shape}. Expected ({x_tr.shape[0]}, 2)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now train a linear regression function on the new data!\n",
    "trsf_w_rls = ...\n",
    "pred_tr = ...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_tr, y_tr, label=\"True\")\n",
    "ax.scatter(x_tr, pred_tr, label=\"Predicted\")\n",
    "ax.legend(loc=\"upper center\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Kernel Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement 3 types of kernel\n",
    "\n",
    "The `kernel_matrix` function takes as input two arrays of data, and outputs the kernel matrix evaluated at every pair of points.\n",
    "\n",
    "You should implement it using the formulas seen in class for the following kernels:\n",
    " - linear kernel -- here the `param` argument can be ignored\n",
    " - polynomial kernel -- here the `param` argument is the exponent of the kernel\n",
    " - gaussian kernel -- here the `param` argument is the kernel length-scale ($\\sigma$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_distances(X1, X2):\n",
    "    \"\"\"Compute the matrix of pairwise squared-distances between all points in X1 and in X2.\n",
    "    \"\"\"\n",
    "    return scipy.spatial.distance.cdist(X1, X2, metric='seuclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put your code here\n",
    "def kernel_matrix(X1, X2, kernel_type, param):\n",
    "    # X1 : array of shape n x d\n",
    "    # X2 : array of shape m x d\n",
    "    if kernel_type == 'linear':\n",
    "        return ...\n",
    "    elif kernel_type == 'polynomial':\n",
    "        exponent = param\n",
    "        return ...\n",
    "    elif kernel_type == 'gaussian':\n",
    "        lengthscale = param\n",
    "        return ...\n",
    "    else:\n",
    "        raise ValueError(kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Visualize the kernel (e.g. of the Gaussian type) for random data with different length-scales. What can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.random.default_rng(42).standard_normal(size=(100, 5))\n",
    "# TODO: put your code here\n",
    "lengthscale = ...\n",
    "K = kernel_matrix(D, D, \"gaussian\", lengthscale)\n",
    "fig, ax = plt.subplots()\n",
    "ax.matshow(K)\n",
    "ax.set_title(f\"Gaussian kernel with sigma={lengthscale:.2f}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Kernel RLS\n",
    "\n",
    "Remember that, given kernel $K = k(x_i, x_j)$ for $i=(1, \\dots, n)$ and $j=(1, \\dots, n)$, KRLS learns a weight-vector with the following formula\n",
    "\n",
    "$$(K + n \\lambda I)w = Y$$\n",
    "\n",
    "and then predictions on some new point $\\tilde{x}$ are given by\n",
    "\n",
    "$$f^{\\mathrm{KRLS}}(\\tilde{x}) = k(\\tilde{x}, X^{\\mathrm{train}}) w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put your code here\n",
    "def krls_train(x, y, reg_par, kernel_type, kernel_par):\n",
    "    w = np.linalg.solve(...)\n",
    "    return w\n",
    "\n",
    "def krls_predict(x_ts, x_tr, w, kernel_type, kernel_par):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the functions you have implemented on the quadratic dataset.\n",
    "\n",
    "**Tasks:**\n",
    " 1. use the linear kernel, can you fit the data?\n",
    " 2. use the polynomial kernel, test the effect of the kernel parameter on the results:\n",
    "    - describe what happens with a low/high exponent in terms of the bias-variance tradeoff.\n",
    " 3. use the polynomial kernel, but fix the kernel parameter. Test the effect of the regularization parameter\n",
    "    - describe what happens with a low/high regularizer in terms of the bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(100, np.array([3]), 2.0)\n",
    "\n",
    "# TODO: put your code here\n",
    "ker_par = ...\n",
    "reg_param = ...\n",
    "ker_type = ...\n",
    "\n",
    "w_krls = ...\n",
    "ypred_tr = ...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_tr, y_tr, label=\"True\")\n",
    "ax.scatter(x_tr, ypred_tr, label=\"Predicted\")\n",
    "ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a more complex non-linear dataset\n",
    "\n",
    "Define a function to generate a d-dimensional synthetic dataset where the targets `Y` depend non-linearly on the variables `X`.\n",
    "\n",
    "The parameters of the function are:\n",
    " - n : the number of samples\n",
    " - d : the dimension of the samples\n",
    " - low_d : the lower-bound for the uniformly distributed samples\n",
    " - high_d : the higher-bound for the uniformly distributed samples\n",
    " - sigma_noise : standard deviation of Gaussian noise added to the targets\n",
    " \n",
    "It should return:\n",
    " - X : 2D array of size n, d which fits in the desired bounds\n",
    " - Y : 2D array of size n, 1 which is a non-linear function of `X` (and a linear function of `w`)\n",
    " \n",
    "Examples of non-linear regression functions:\n",
    " - polynomial dependence of the Y on the X data\n",
    " - logarithmic dependence\n",
    " - more complex transforms (e.g. trig functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_data_gen(n, d, low_d, high_d, sigma_noise, rng=None):\n",
    "    rng = _check_random_generator(rng)\n",
    "\n",
    "    X = rng.uniform(low=low_d, high=high_d, size=(n, d))\n",
    "    assert X.shape == (n, d), \"Shape of X incorrect\"\n",
    "\n",
    "    # TODO: put your code here\n",
    "    Y = ...\n",
    "    assert Y.shape == (n, 1), \"Shape of Y incorrect\"\n",
    "\n",
    "    # Add noise\n",
    "    noise = rng.normal(0, sigma_noise, size=(n, 1))\n",
    "    Y_noisy = Y + noise\n",
    "\n",
    "    return X, Y_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRLS and cross-validation\n",
    "Now we will use KRLS and k-fold CV to learn on the complex non-linear datasets we have generated.\n",
    "\n",
    "In this last part you will have to do the following:\n",
    " 1. Generate a non-linear dataset, find the best hyperparameters (using e.g. the Gaussian kernel) with the training set, and then use them to make predictions on the test-set. Are you able to fit your data well?\n",
    " 2. Analyse how the amount of noise (see the `sigma_noise` parameter of `nonlinear_data_gen`) influences the best lambda as selected by cross-validation. In particular answer to the following question:\n",
    "     - How does the best lambda change as you increase/decrease the amount of noise in your dataset? Why? \n",
    "     \n",
    "    **Hint: keep the kernel parameter fixed for this third task, otherwise it might be very hard to see**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _make_kfold_indices(n_tot, num_folds, rng):\n",
    "    perm = rng.choice(n_tot, size=n_tot, replace=False)\n",
    "    return np.array_split(perm, num_folds)\n",
    "\n",
    "\n",
    "def krls_kfold_valerr(x_tr, y_tr, split_idx, reg_par, kernel_type, kernel_par):\n",
    "    \"\"\"\n",
    "    Compute train/val errors over *given* folds (split_idx is a list of index arrays).\n",
    "    \"\"\"\n",
    "    n_tot = x_tr.shape[0]\n",
    "    assert len(split_idx) > 1 and all(idx.dtype == int for idx in split_idx)\n",
    "    assert sum(len(idx) for idx in split_idx) == n_tot\n",
    "\n",
    "    tr_errs, val_errs = [], []\n",
    "    for val_ids in split_idx:\n",
    "        val_mask = np.zeros(n_tot, dtype=bool)\n",
    "        val_mask[val_ids] = True\n",
    "\n",
    "        kf_x_tr, kf_y_tr = x_tr[~val_mask], y_tr[~val_mask]\n",
    "        kf_x_val, kf_y_val = x_tr[val_mask], y_tr[val_mask]\n",
    "\n",
    "        w_krls = krls_train(kf_x_tr, kf_y_tr, reg_par=reg_par, kernel_type=kernel_type, kernel_par=kernel_par)\n",
    "        pred_tr  = krls_predict(kf_x_tr,  kf_x_tr, w_krls, kernel_type=kernel_type, kernel_par=kernel_par)\n",
    "        pred_val = krls_predict(kf_x_val, kf_x_tr, w_krls, kernel_type=kernel_type, kernel_par=kernel_par)\n",
    "\n",
    "        tr_errs.append(calc_err(pred_tr,  kf_y_tr))\n",
    "        val_errs.append(calc_err(pred_val, kf_y_val))\n",
    "\n",
    "    return np.asarray(tr_errs), np.asarray(val_errs)\n",
    "\n",
    "\n",
    "def krls_kfoldcv(x_tr, y_tr, num_folds, reg_par_list, kernel_type, kernel_par_list, rng=None):\n",
    "    \"\"\"\n",
    "    Choose the best parameters for both the regularizer and the kernel parameter according to K-Fold CV.\n",
    "    \"\"\"\n",
    "    n_tot = x_tr.shape[0]\n",
    "    assert num_folds > 1 and num_folds <= n_tot\n",
    "\n",
    "    # Precompute shared folds once for fair comparison\n",
    "    rng = _check_random_generator(rng)\n",
    "    split_idx = _make_kfold_indices(n_tot, num_folds, rng)\n",
    "\n",
    "    errors = np.zeros((len(reg_par_list), len(kernel_par_list)))\n",
    "    for i, reg_par in enumerate(reg_par_list):\n",
    "        for j, kernel_par in enumerate(kernel_par_list):\n",
    "            _, val_error = krls_kfold_valerr(x_tr, y_tr, split_idx, reg_par, kernel_type, kernel_par)\n",
    "            errors[i, j] = np.mean(val_error)\n",
    "\n",
    "    best_i, best_j = np.unravel_index(np.argmin(errors), errors.shape)\n",
    "    best_reg_par = reg_par_list[best_i]\n",
    "    best_kernel_par = kernel_par_list[best_j]\n",
    "    best_err = errors[best_i, best_j]\n",
    "\n",
    "    print(f\"The best error (MSE={best_err:.5f}) was obtained with \"\n",
    "          f\"lambda={best_reg_par}, kernel parameter={best_kernel_par}\")\n",
    "\n",
    "    return best_reg_par, best_kernel_par, best_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data gen parameters\n",
    "n_tr = 200\n",
    "n_test = 100\n",
    "d = 1\n",
    "low_d = -2\n",
    "high_d = 2\n",
    "sigma_noise = 0.1\n",
    "\n",
    "# TODO: generate datasets\n",
    "x_tr, y_tr = ...\n",
    "x_test, y_test = ...\n",
    "\n",
    "# K_fold parameters\n",
    "num_folds = 5\n",
    "reg_par_list = [0.001, 0.01, 0.1, 1, 10, 50, 100, 500]\n",
    "ker_type = 'gaussian'\n",
    "ker_par_list = [0.01 * (10**i) for i in range(5)]\n",
    "\n",
    "# TODO: Compute HERE the best kernel parameter and the best regularization parameter (use krls_kfoldcv!)\n",
    "best_reg_par, best_kernel_par, _ = ...\n",
    "\n",
    "# TODO: Now retrain on the whole of x_tr with the best parameters and test on x_ts!\n",
    "w_krls = ...\n",
    "y_pred = ...\n",
    "test_error = calc_err(y_pred, y_test)\n",
    "print(f'The MSE on the test data is: {test_error:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyse the effect of noise!\n",
    "for noise in [0.001, 0.01, 0.1, 0.2, 0.5]:\n",
    "    print(f'Noise {noise}:')\n",
    "    x_tr, y_tr = ...\n",
    "    x_test, y_test = ...\n",
    "\n",
    "    # TODO: Use krls_kfoldcv\n",
    "    best_reg_par, best_kernel_par, _ = ...\n",
    "\n",
    "    # TODO: Now retrain on the whole of x_tr with the best parameters and test on x_ts!\n",
    "    w_krls = ...\n",
    "    y_pred = ...\n",
    "    test_error = calc_err(y_pred, y_test)\n",
    "    print(f'The MSE on the test data is: {test_error:.5f}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
