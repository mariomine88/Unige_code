{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0d277b",
   "metadata": {},
   "source": [
    "# Lab. 5 - A glimpse of Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5470c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeca772",
   "metadata": {},
   "source": [
    "## Instructions on using Keras\n",
    "\n",
    "Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "We create a [Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) object which holds multiple layers executed one by one, and add layers to the object until we have formed the architecture.\n",
    "\n",
    "To create the first layer, you need to know the right number of input features. You can specify this by using the `input_dim` argument. \n",
    "\n",
    "The things to choose when defining the architecture are many:\n",
    " - number of layers\n",
    " - type of layers\n",
    " - size of layers\n",
    " - type of non-linearity\n",
    " - whether or not to add regularization\n",
    " \n",
    "Here we will use only fully-connected (dense) layers, so the type of layer is fixed. Fully connected layers are defined using the [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) class, which takes as parameters the number of neurons (which is the **dimension of the output**).\n",
    "\n",
    "The activation functions are used after each dense layer. You can choose the activation functions for hidden layers yourself, a common choice being the [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu) activation. But for the last layer, the activation must reflect the range of the outputs.\n",
    "\n",
    "Since we will work with binary classification problem, the output should be between 0 and 1, which is then easy to map to any given class. To ensure this we can use the [Sigmoid](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid) activation.\n",
    "\n",
    "\n",
    "After having created a model you need to **compile** it. During the compilation phase you must specify some parameters related to how the model will be optimized:\n",
    " - The `optimizer`. For the following exercise you should use [SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD), initialized with some learning rate (instructions on how to choose it follow).\n",
    " - The `loss` function. For binary classification you can use the [cross-entropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) loss.\n",
    " - A list of `metrics`: common error functions which you want keras to report at each training epoch.\n",
    " \n",
    "Then you may actually train the model by calling **fit**. The fit function takes as input the training data, and some more parameters related to the training process:\n",
    " - `epochs` : the number of epochs to train for\n",
    " - `batch_size` : the size of mini-batches. A high batch-size will speed up computations but may make training unstable.\n",
    " \n",
    "Other useful functions are `model.predict` which runs the model's forward pass to predict on new samples, and `model.evaluate` which is similar to `predict` but instead of giving predictions as output, it simply computes some metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4009e7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generate_2d_cls_data(n, sigma_noise):\n",
    "    n_cls = n // 2\n",
    "\n",
    "    theta = np.random.randn(n_cls) * 2 * np.pi\n",
    "    cls1 = np.stack([np.cos(theta) * 2, np.sin(theta) * 2], axis=1)\n",
    "    cls2 = np.random.randn(n_cls, 2) * 0.5\n",
    "\n",
    "    cls1 += np.random.randn(cls1.shape[0], 2) * sigma_noise * 3\n",
    "    cls2 += np.random.randn(cls2.shape[0], 2) * sigma_noise\n",
    "\n",
    "    X = np.concatenate([cls1, cls2], axis=0)\n",
    "    y = np.concatenate([np.zeros(n_cls), np.ones(n_cls)], axis=0)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d91f77",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def separatingFLR(data, labels, predictions, model):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    num_grid_points = 100\n",
    "    xi = np.linspace(data[:, 0].min(), data[:, 0].max(), num_grid_points)\n",
    "    yi = np.linspace(data[:, 1].min(), data[:, 1].max(), num_grid_points)\n",
    "    gdata = np.stack([xi, yi], 1)\n",
    "    X, Y = np.meshgrid(xi,yi)\n",
    "    pred_grid = model.predict(\n",
    "        np.stack([X.reshape(-1), Y.reshape(-1)], axis=1)\n",
    "    ).reshape(num_grid_points, num_grid_points)\n",
    "\n",
    "    ax.contour(xi, yi, pred_grid, 15, linewidths=2, colors='k', levels=[0.5])\n",
    "    # plot data points.\n",
    "    ax.scatter(data[:,0], data[:,1], c=labels.ravel(), marker='o', s=100, zorder=10, alpha=0.8)\n",
    "    ax.set_xlim(data[:,0].min(), data[:,0].max())\n",
    "    ax.set_ylim(data[:,1].min(), data[:,1].max())\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71125f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def calc_err(predicted, observed):\n",
    "    predicted = predicted.ravel()\n",
    "    observed = observed.ravel()\n",
    "    threshold_preds = predicted.copy()\n",
    "    threshold_preds[predicted < 0.5] = 0\n",
    "    threshold_preds[predicted >= 0.5] = 1\n",
    "    return np.mean(threshold_preds != observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3381dd",
   "metadata": {},
   "source": [
    "### Synthetic Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81cbd64",
   "metadata": {},
   "source": [
    "#### Generating the train and test sets\n",
    "\n",
    "Note that with neural nets, the labels for binary classification should be 0 and +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff6be4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build your first model by creating a Sequential object and then adding 2 Dense layers:\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # This is the last layer, it should have 1 neuron and the sigmoid activation\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.05), # Algorithm used for optimization\n",
    "    loss='binary_crossentropy',                         # The loss function\n",
    "    metrics=['accuracy'],                               # Metrics to evaluate the goodness of predictions\n",
    ")\n",
    "history = model.fit(\n",
    "    Xtr, Ytr,                       # Training data\n",
    "    epochs=600,                     # Number of training epochs\n",
    "    batch_size=10,                  # Train using mini-batches of 10 samples each\n",
    "    validation_split=0.2,           # Split the data using 80% to train and 20% for validation\n",
    ")\n",
    "\n",
    "# Predict and calculate errors\n",
    "train_preds = model.predict(Xtr)\n",
    "test_preds = model.predict(Xte)\n",
    "train_err = calc_err(train_preds, Ytr)\n",
    "test_err = calc_err(test_preds, Yte)\n",
    "print(\"Training error: %.2f%%, Test error: %.2f%%\" % (train_err * 100, test_err * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7362e",
   "metadata": {},
   "source": [
    "#### Plot the results from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57874cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this function you can plot the history of the model training produced by the fit function\n",
    "def plot_history(history):\n",
    "    fig, ax = plt.subplots()\n",
    "    # Plot training & validation accuracy values\n",
    "    ax.plot(history.history['accuracy'], label='Train')\n",
    "    ax.plot(history.history['val_accuracy'], label='Val')\n",
    "    ax.set_title('Model accuracy')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='best')\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ba8d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = separatingFLR(Xtr, Ytr, train_preds, model)\n",
    "ax.set_title(\"Predicted class seperation\");\n",
    "fig, ax = plot_history(history)\n",
    "ax.set_title(\"Accuracy evolution over epochs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037ce43",
   "metadata": {},
   "source": [
    "## Exploring Different Architectures\n",
    "\n",
    "Using a simple binary classification dataset you will build a keras model with Dense layers, and the RELU activation function.\n",
    "\n",
    "You will explore different architectures to try and see when the NN overfits or underfits the data.\n",
    "\n",
    "In particular, you should try the following:\n",
    " 1. A NN with a single hidden layer with many (e.g. 100) neurons\n",
    " 2. A NN with many (e.g. 3, 4, 5) hidden layers with a few neurons (e.g. 10 to 30).\n",
    " \n",
    "Train the neural network using the SGD algorithm with a learning rate of 0.05 (you may explore different values) for 500 epochs (or less if time doesn't permit).\n",
    "\n",
    "For each setting you try, plot the training and validation errors as a function of the epochs, and plot the separating function (use the `separatingFLR` function).\n",
    "\n",
    "For each setting comment on whether the NN is overfitting or not. Further comment on which model you believe is better, and why (e.g. computational or accuracy considerations).\n",
    "\n",
    "We provide some skeleton code to train and evaluate a model, you'll have to fill it in, and do the same thing multiple times for different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98dfab",
   "metadata": {},
   "source": [
    "#### 1) NN with single layer with 100 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your first model by creating a Sequential object and then adding 3 Dense layers:\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "# TODO: Put your code here\n",
    "...\n",
    "\n",
    "# Fit model\n",
    "# TODO: Put your code here\n",
    "history = ...\n",
    "\n",
    "# Predict and calculate errors\n",
    "train_preds = model.predict(Xtr)\n",
    "test_preds = model.predict(Xte)\n",
    "train_err = calc_err(train_preds, Ytr)\n",
    "test_err = calc_err(test_preds, Yte)\n",
    "print(f\"Training error: {train_err*100: .2f}%, Test error: {test_err*100: .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot separating function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb721110",
   "metadata": {},
   "source": [
    "#### 2) NN with 4 hidden layer with  40 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1383aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the previous code but with 4 hidden layers with 40 neurons (in each layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot separating function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb1ffd5",
   "metadata": {},
   "source": [
    "## [Optional] Train a Neural Network on MNIST!\n",
    "\n",
    "We can finally switch to a real dataset now.\n",
    "\n",
    "First we will load the MNIST dataset and plot it to see how it really looks like.\n",
    "\n",
    "Then we will use the notions learned in the first part to train a model which can distinguish two digits in the MNIST data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba03c09",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc461f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(\"Training set shapes:\", x_train.shape, y_train.shape)\n",
    "print(\"Test set shapes:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5262e29",
   "metadata": {},
   "source": [
    "#### Bring the data into shape\n",
    "\n",
    "1. Choose the digits we want to classify (variables `num_1` and `num_2`)\n",
    "2. Then we restrict training and test sets to only use those numbers\n",
    "3. We reshape the images from 28*28 to a single 784-dimensional vector\n",
    "4. Finally we modify the labels to their appropriate range (0 and +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e5f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick two digits, later you can try with different pairs\n",
    "num_1 = 1\n",
    "num_2 = 7\n",
    "\n",
    "# Here we take only two digits from MNIST. \n",
    "# We will reduce the problem to binary classification.\n",
    "Xtr = x_train[(y_train == num_1) | (y_train == num_2)]\n",
    "Ytr = y_train[(y_train == num_1) | (y_train == num_2)]\n",
    "Xts = x_test[(y_test == num_1) | (y_test == num_2)]\n",
    "Yts = y_test[(y_test == num_1) | (y_test == num_2)]\n",
    "\n",
    "# Reshape the data correctly\n",
    "Xtr = Xtr.reshape(-1, 28*28)\n",
    "Xts = Xts.reshape(-1, 28*28)\n",
    "Ytr[Ytr == num_1] = 0\n",
    "Ytr[Ytr == num_2] = 1\n",
    "Yts[Yts == num_1] = 0\n",
    "Yts[Yts == num_2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4ef62",
   "metadata": {},
   "source": [
    "We can visualize the images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(Xtr[3].reshape(28, 28))\n",
    "ax.set_title(Ytr[3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a6381",
   "metadata": {},
   "source": [
    "#### Define The Keras Model\n",
    "\n",
    "Here you can play with the structure of the network, e.g. you may start from dense layers and then check what happens if you replace them with convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Add hidden layers\n",
    "# TODO: Put your code here\n",
    "...\n",
    "# This is the last layer, it should have 1 neuron and the sigmoid activation\n",
    "model.add(Dense(1, activation='sigmoid'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec3861d",
   "metadata": {},
   "source": [
    "#### Compile The Keras Model\n",
    "\n",
    "Here we will use the Adam optimizer instead. It tends to work better than SGD with high dimensional data (such as our MNIST images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea72d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33d1ed",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674a529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the keras model on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2994f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and calculate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the keras model. Is this consistent with the error you computed above?\n",
    "_, accuracy = model.evaluate(Xts, Yts)\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
