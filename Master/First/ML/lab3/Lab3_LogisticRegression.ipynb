{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Logistic Regression\n",
    "\n",
    "In this lab we implement and use logistic regressione for binary claffication problems.\n",
    "\n",
    "We start including some libraries and functions already seen in the previous labs (or slight variations of them). Have a look and verify you understand their purpose.\n",
    "\n",
    "<b>READ all the text parts very carefully, as you will find instructions on how to proceed.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import Generator\n",
    "\n",
    "\n",
    "def _check_random_generator(rng: int | Generator | None) -> Generator:\n",
    "    \"\"\"Convert rng into a np.random.Generator instance.\"\"\"\n",
    "    if rng is None:\n",
    "        print(\"⚠️  Warning: Set the `rng` parameter to a NumPy random generator\\n\"\n",
    "              \"   (e.g. `np.random.default_rng(42)`) or specify a fixed seed to ensure\\n\"\n",
    "              \"   your results are reproducible. Proceeding with a random seed for now.\")\n",
    "        return np.random.default_rng()\n",
    "    if isinstance(rng, np.random.Generator):\n",
    "        return rng\n",
    "    if isinstance(rng, (int, np.integer)):\n",
    "        return np.random.default_rng(rng)\n",
    "\n",
    "\n",
    "def mixGauss(means, sigmas, n, rng=None):\n",
    "    \"\"\"\n",
    "    means : 2D array (num_classes, d)\n",
    "        Each row of the array gives the mean of the Gaussian in multiple dimensions for one class.\n",
    "        For binary classification problems, the number of rows should be 2!\n",
    "    sigmas : 1D array (num_classes)\n",
    "        The standard deviation for the Gaussian distribution of each class (isotropic Gaussian!)\n",
    "    n : int (num_elements)\n",
    "        Number of samples to generate\n",
    "\n",
    "    Example:\n",
    " \n",
    "    >>> means = [[3, 0], [0, 0]]\n",
    "    >>> sigmas = [0.5, 1]\n",
    "    >>> X, Y = mixGauss(means, sigmas, n=100)\n",
    "    >>> fig, ax = plt.subplots()\n",
    "    >>> ax.scatter(X[Y == 1,0], X[Y == 1,1], marker='o', color='r')\n",
    "    >>> ax.scatter(X[Y == -1,0], X[Y == -1,1], marker='o', color='b')\n",
    "    \"\"\"\n",
    "    rng = _check_random_generator(rng)\n",
    "\n",
    "    means = np.asarray(means)\n",
    "    sigmas = np.asarray(sigmas)\n",
    "\n",
    "    num_classes = sigmas.shape[0]\n",
    "    assert means.shape[0] == num_classes, \"Number of `means` and `sigmas` should be the same.\"\n",
    "\n",
    "    d = means.shape[1]\n",
    "    data = np.full((n * num_classes, d), np.inf)\n",
    "    labels = np.zeros(n * num_classes, dtype=np.int64)\n",
    "\n",
    "    for idx, sigma in enumerate(sigmas):\n",
    "        data[idx * n:(idx + 1) * n] = rng.multivariate_normal(\n",
    "            mean=means[idx], cov=np.eye(d) * sigma ** 2, size=n)\n",
    "        labels[idx * n:(idx + 1) * n] = idx\n",
    "\n",
    "    if(num_classes == 2):\n",
    "        labels[labels==0] = -1\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize random dataset\n",
    "X, Y = mixGauss([[1, 1], [0, 0]], [1, 0.2], n=200)\n",
    "plt.scatter(X[Y==-1, 0], X[Y==-1, 1], c='r')\n",
    "plt.scatter(X[Y==1, 0], X[Y==1, 1], c='b');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipLabels(Y, perc, rng=None):\n",
    "    assert 0 <= perc <= 100, \"`perc` should be a percentage value between 0 and 100.\"\n",
    "    assert all(np.abs(Y) == 1), \"The values of Ytr should be +1 or -1.\"\n",
    "\n",
    "    Y_noisy = np.copy(np.squeeze(Y))\n",
    "    assert Y_noisy.ndim == 1, \"Please supply a label array with only one dimension\"\n",
    "    n = Y_noisy.shape[0]\n",
    "\n",
    "    rng = _check_random_generator(rng)\n",
    "    n_flips = int(np.floor(n * perc / 100))\n",
    "    idx_to_flip = rng.choice(n, size=n_flips, replace=False)\n",
    "    Y_noisy[idx_to_flip] = -Y_noisy[idx_to_flip]\n",
    "\n",
    "    return Y_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear logistic regression with gradient descent\n",
    "\n",
    "We define two functions:\n",
    " - the `optimal_gd_learning_rate(Xtr, reg_par)` function calculates the optimal learning rate for GD on a given dataset. You will need to use the SVD of the covariance matrix.\n",
    " - the `train_logreg_gd(Xtr, Ytr, reg_par, maxiter)` function estimates the classifier weights on the training set.\n",
    "\n",
    "<br>\n",
    "\n",
    "The parameters of the `optimal_gd_learning_rate` function are:\n",
    "- <b>Xtr</b> is the nxD matrix of training set inputs\n",
    "- <b>reg_par</b> is the value of the lammbda\n",
    "\n",
    "and it should output the learning rate $\\gamma$ (a scalar).\n",
    "You should use the `np.linalg.eigvalsh` function to calculate the eigenvalues of the covariance matrix.\n",
    "\n",
    "<br>\n",
    "\n",
    "The parameters of the `train_logreg_gd` function are:\n",
    "- <b>Xtr</b> is the nxD matrix of training set inputs\n",
    "- <b>Ytr</b> is the n vector of training set outputs\n",
    "- <b>reg_par</b> is the value of the lammbda\n",
    "- <b>maxiter</b> is the maximum number of iterations to run gradient descent\n",
    "\n",
    "and it should output:\n",
    "- <b>w</b> is the D vector of the estimated function parameters\n",
    "- <b>losses</b> is the vector of the loss at each iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the gamma parameter: the optimal learning rate for gradient descent\n",
    "def optimal_gd_learning_rate(Xtr, reg_par):\n",
    "    eigvals = np.linalg.eigvalsh(Xtr.T @ Xtr)\n",
    "    # Calculate L: the Lipschitz constant of the gradient\n",
    "    L = np.max(eigvals) / Xtr.shape[0] + 2 * reg_par\n",
    "    gamma = 1 / L\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg_gd(Xtr, Ytr, reg_par, epsilon = 1e-6, maxiter=100):\n",
    "    \"\"\"\n",
    "    Xtr : array of shape n, d\n",
    "    Ytr : array of shape n, or of shape n, 1\n",
    "    reg_par : regularization parameter (a scalar)\n",
    "    epsilon :  the tolerance for the stopping criterion\n",
    "    maxiter : the maximum number of gradient-descent iterations\n",
    "    \"\"\"\n",
    "    # size of the input in the training\n",
    "    _, D = np.shape(Xtr)\n",
    "\n",
    "    # initialization of the vector w\n",
    "    w = np.zeros((D, 1))\n",
    "\n",
    "    # Set the learning rate optimally\n",
    "    gamma = optimal_gd_learning_rate(Xtr, reg_par)\n",
    "\n",
    "    # initialization of some supporting variables\n",
    "    j = 0\n",
    "    loss_old = 0\n",
    "    loss = float(\"inf\")\n",
    "    training_losses = np.zeros(maxiter + 1)\n",
    "    Ytr = Ytr.reshape(-1, 1)  # Convert from shape n, to shape n, 1\n",
    "    while j < maxiter and abs(loss - loss_old) >= epsilon:\n",
    "        loss_old = loss\n",
    "\n",
    "        # TODO: Update the weights\n",
    "        w = w - gamma * (...)\n",
    "        # TODO: Compute the loss\n",
    "        loss = ...\n",
    "        training_losses[j] = loss\n",
    "        j = j + 1\n",
    "    return w, training_losses[:j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation the function on the test set\n",
    "\n",
    "A function to perform predictions on a set of samples given the learned logistic regression weights\n",
    "\n",
    "##### Ypred, Ppred = predict_logreg(weights, Xte)\n",
    "where\n",
    "- <b>weights</b> is the D vector of the estimated function parameters\n",
    "- <b>X</b> is the matrix of input points of the training or test set.\n",
    "- <b>Ypred</b> is the vector of predictions\n",
    "- <b>Ppred</b> is the predicted probability of a point belonging to class +1. It will be 0 if the model is very confident the point belongs to class -1, it will be 1 if the model is very confident that the point belongs to class +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logreg(weights, X):\n",
    "    \"\"\"\n",
    "    weights : array of shape d, 1\n",
    "    X : array of shape n, d\n",
    "    \"\"\"\n",
    "    ypred = np.dot(X, weights)\n",
    "    # TODO: Try and understand what it does, deriving the formula\n",
    "    ppred = 1 / (1 + np.exp(-ypred))\n",
    "    # The outputs are reshaped to be 1D vectors\n",
    "    return np.sign(ypred).reshape(-1), ppred.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcError(Ypred, Y):\n",
    "    return np.mean(Ypred != Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Analysis\n",
    "\n",
    "Perform the following tasks for the initial analysis:\n",
    "\n",
    "1. Create two binary classification datasets (training and test sets) -- Use the same parameters. Visualize them in a scatter plot.\n",
    "\n",
    "2. Pick a reasonable value for lambda (e.g. reg_par = 0.1, 0.01, 0.001, ...) and train a logistic-regression model using the functions you have defined.\n",
    "\n",
    "3. Plot the loss at each iteration which is returned by the training function. The loss should decrease at each iteration, **if the loss does not decrease there is an error in the implementation!**\n",
    "\n",
    "4. Use the `separatingFLR` function to plot the separating curve obtained with the model\n",
    "\n",
    "5. Evaluate the error training and test sets.\n",
    "\n",
    "**Important #1**: The parameters used to generate the data are quite important. Try to make sure that the two classes are distinct (i.e. the means of the Gaussians should be different), but also not too far. Ideally a few points should overlap between the classes.\n",
    "Make sure to generate at least 100 points for both train and test sets.\n",
    "\n",
    "**Important #2**: since we are implementing a linear model, we must add a bias term -- otherwise the weights will draw a line which always goes through 0. An alternative to adding a bias term to the model is to add a feature comprising all ones to the data-column. We can use the following code for this:\n",
    "```\n",
    "Xtr_wbias = np.hstack((Xtr, np.ones((Xtr.shape[0], 1))))\n",
    "Xts_wbias = np.hstack((Xts, np.ones((Xts.shape[0], 1))))\n",
    "```\n",
    "what whill be the resulting shape of the weight vector `w`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the separating curve for the calculated weights w\n",
    "def separatingFLR(Xtr, Ytr, w):\n",
    "    x1min, x1max = min(Xtr[:, 0]), max(Xtr[:, 0])\n",
    "    x2min, x2max = min(Xtr[:, 1]), max(Xtr[:, 1])\n",
    "\n",
    "    # Number of meshpoints for visualization\n",
    "    Nx1, Nx2 = 1000, 1000\n",
    "    x1mesh = np.linspace(x1min, x1max, Nx1)\n",
    "    x2mesh = np.linspace(x2min, x2max, Nx2)\n",
    "\n",
    "    # Create mesh and reshape x1 and x2 into a single array of points\n",
    "    x1, x2 = np.meshgrid(x1mesh, x2mesh)\n",
    "    meshpoints = np.column_stack((x1.ravel(), x2.ravel()))\n",
    "    meshpoints_bias = np.hstack((meshpoints, np.ones((meshpoints.shape[0], 1))))\n",
    "\n",
    "    # TODO: Calculate the predicted labels for `meshpoints_bias`\n",
    "    ypred = (...)\n",
    "    ymesh = np.reshape(ypred, (Nx2, Nx1))\n",
    "\n",
    "    # Plot data points\n",
    "    _, ax = plt.subplots()\n",
    "    ax.contour(x1mesh, x2mesh, ymesh, levels=[0], colors='red')\n",
    "    ax.scatter(Xtr[:,0], Xtr[:,1], s=70, c=Ytr, alpha=0.8)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help for data-generation\n",
    "means = ...\n",
    "sigmas = ...\n",
    "\n",
    "Xtr, Ytr = ...\n",
    "Xts, Yts = ...\n",
    "\n",
    "Xtr_wbias = ... \n",
    "Xts_wbias = ...\n",
    "\n",
    "# TODO: Plot training and test sets, coloring the two classes differently.\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "ax0 = fig.add_subplot(1, 2, 1)\n",
    "ax1 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "ax0.set_title(\"Training set\")\n",
    "ax0.scatter(Xtr[:,0], Xtr[:,1], s=70, c=Ytr, alpha=0.8)\n",
    "\n",
    "ax1.set_title(\"Test set\")\n",
    "ax1.scatter(Xts[:,0], Xts[:,1], s=70, c=Yts, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the separating curve (on the test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the errors\n",
    "tr_pred, _ = predict_logreg(...)\n",
    "tr_err = calcError(...)\n",
    "ts_pred, _ = predict_logreg(...)\n",
    "ts_err = calcError(...)\n",
    "print(f\"Training error: {tr_err*100:.2f}, Test error: {ts_err*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the confidence of logistic regression predictions\n",
    "\n",
    "Logistic regression does not only output *pointwise predictions* (the class to which a point belongs), but it also gives the **probability** that a test point belongs to a certain class.\n",
    "\n",
    "This probability can be very useful to interpret the outputs of your model: in certain cases it might be better to **not predict anything** if the confidence of the model is low (i.e. if the model predicts a probability of 0.5 in a binary setting, the model is not sure which class a point belongs to).\n",
    "\n",
    "In this part of the lab, we will\n",
    " 1. Implement a function which allows to visualize the confidence of predictions (`plot_logreg_confidence`)\n",
    " 2. Train a logistic regression model, and use the visualization function to see where the low confidence region of prediction lies.\n",
    " 3. Calculate the error on **just the high-confidence** predictions, and see how the it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logreg_confidence(X, Y, pred_confidence, threshold):\n",
    "    \"\"\"\n",
    "    X : a (n, d) dataset\n",
    "    Y : a (n, ) array of targets\n",
    "    pred_confidence : A (n, ) array of probabilities predicted from X\n",
    "    threshold : a float between 0 and 0.5 determining the probability threshold we use to \n",
    "                consider neutral predictions. For example if threshold=0.1 then all probabilities\n",
    "                between 0.4 and 0.6 will be considered neutral (that is, neither belonging to class +1\n",
    "                or to class -1).\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots()\n",
    "    xi = np.linspace(X[:, 0].min(), X[:, 0].max(), 200)\n",
    "    yi = np.linspace(X[:, 1].min(), X[:, 1].max(), 200)\n",
    "    X_grid, Y_grid = np.meshgrid(xi,yi)\n",
    "\n",
    "    zi = griddata(X, pred_confidence, (X_grid, Y_grid), method='linear')\n",
    "\n",
    "    ax.contour(xi, yi, zi, 15, linewidths=2, levels=[0.5 - threshold, 0.5 + threshold])\n",
    "\n",
    "    # Plot data points\n",
    "    ax.scatter(X[:,0], X[:,1], c=Y, marker='o', s=100, zorder=10, alpha=0.8)\n",
    "    ax.set_xlim(X[:,0].min(), X[:,0].max())\n",
    "    ax.set_ylim(X[:,1].min(), X[:,1].max())\n",
    "    plot.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "Xtr, Ytr = mixGauss([[0,1],[1,1]], [0.4,0.4], 100)\n",
    "Xts, Yts = mixGauss([[0,1],[1,1]], [0.4,0.4], 100)\n",
    "\n",
    "Xtr_wbias = np.hstack((Xtr, np.ones((Xtr.shape[0], 1))))\n",
    "Xts_wbias = np.hstack((Xte, np.ones((Xte.shape[0], 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a logistic regression model, and obtain the probability predictions for the test set\n",
    "...\n",
    "ts_pred, ts_conf = predict_logreg(...)\n",
    "# TODO: Plot the logistic regression confidence for different thresholds. Interpret what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error_with_confidence(prob_pred, Y, threshold):\n",
    "    \"\"\"\n",
    "    Calculate the classification error on only the predictions with a high confidence!\n",
    "    \"\"\"\n",
    "    class_m1 = prob_pred <= 0.5 - threshold\n",
    "    class_p1 = prob_pred > 0.5 + threshold\n",
    "\n",
    "    err_class_m1 = Y[class_m1] != -1\n",
    "    err_class_p1 = Y[class_p1] != 1\n",
    "    return (np.sum(err_class_m1) + np.sum(err_class_p1)) / (len(err_class_m1) + len(err_class_p1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a log-reg model, and calculate the error on only the high-confidence examples from the test\n",
    "#       set. How do you expect this error to behave as you change the threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Stochastic Gradient Descent\n",
    "\n",
    "SGD is quite similar to GD, but instead of updating the weights with the gradient of all the training samples at once, at each step it updates the weights with the **gradient of a single sample** picked at random from the training set.\n",
    "\n",
    "Since at each iteration it uses a single sample it will be much faster, but it will need more iterations.\n",
    "\n",
    "In this last part you will:\n",
    " 1. Implement a SGD solver for logistic regression\n",
    " 2. Use it to learn a logistic-regression model\n",
    " 3. Compare the results to learning with GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg_sgd(Xtr, Ytr, reg_par, epsilon = 1e-6, maxiter=100):\n",
    "    \"\"\"\n",
    "    Xtr : array of shape n, d\n",
    "    Ytr : array of shape n, or of shape n, 1\n",
    "    reg_par : regularization parameter (a scalar)\n",
    "    epsilon :  the tolerance for the stopping criterion\n",
    "    maxiter : the maximum number of gradient-descent iterations\n",
    "    \"\"\"\n",
    "    # size of the input in the training\n",
    "    n, D = np.shape(Xtr)\n",
    "    # initialization of the vector w\n",
    "    w = np.zeros((D, 1))\n",
    "    \n",
    "    # initialization of some supporting variables\n",
    "    training_losses = np.zeros(maxiter)\n",
    "    Ytr = Ytr.reshape(-1, 1)  # Convert from shape n, to shape n, 1\n",
    "    for j in range(maxiter):\n",
    "        # The learning rate here is different from GD. You can take\n",
    "        # a) 1 / sqrt(n)\n",
    "        # b) 1 / sqrt(j) : the learning rate decreases at each iteration.\n",
    "        # Which one do you expect leads to faster convergence?\n",
    "        gamma = 1 / np.sqrt(n)\n",
    "\n",
    "        # TODO: Choose the sample current sample from Xtr **at random**\n",
    "        sample_idx = ...\n",
    "\n",
    "        # TODO: Use the formulas from the slides to update the weight vector\n",
    "        w = w - gamma * (...)\n",
    "        # TODO: Calculate the loss on the current sample\n",
    "        loss = ...\n",
    "        training_losses[j] = loss\n",
    "\n",
    "        if j > 0 and abs(training_losses[j] - training_losses[j-1]) < epsilon:\n",
    "            break\n",
    "\n",
    "    return w, training_losses[:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate a dataset (use the code from the previous parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train logistic regression with SGD\n",
    "# TODO: Train logistic regression with GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare the two in terms of:\n",
    "#  1. number of iterations needed to converge (keep in mind that GD iterations cost n times more)\n",
    "#  2. accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
