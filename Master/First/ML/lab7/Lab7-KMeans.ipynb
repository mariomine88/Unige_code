{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. 8 - K-Means and the LLoyd algorithm\n",
    "\n",
    "In this lab we consider the problem of **unsupervised learning**, through one of the most famous **clustering** algorithms: K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_random_generator(rng):\n",
    "    \"\"\"Convert rng into a np.random.Generator instance.\"\"\"\n",
    "    if rng is None:\n",
    "        print(\"⚠️  Warning: Set the `rng` parameter to a NumPy random generator\\n\"\n",
    "              \"   (e.g. `np.random.default_rng(42)`) or specify a fixed seed to ensure\\n\"\n",
    "              \"   your results are reproducible. Proceeding with a random seed for now.\")\n",
    "        return np.random.default_rng()\n",
    "    if isinstance(rng, np.random.Generator):\n",
    "        return rng\n",
    "    if isinstance(rng, (int, np.integer)):\n",
    "        return np.random.default_rng(rng)\n",
    "\n",
    "\n",
    "random_seed = 42\n",
    "rng = np.random.default_rng(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixGauss(means, sigmas, n, rng=None):\n",
    "    \"\"\"\n",
    "    means : 2D array (num_classes, d)\n",
    "        Each row of the array gives the mean of the Gaussian in multiple dimensions for one class.\n",
    "        For binary classification problems, the number of rows should be 2!\n",
    "    sigmas : 1D array (num_classes)\n",
    "        The standard deviation for the Gaussian distribution of each class (isotropic Gaussian!)\n",
    "    n : int (num_elements)\n",
    "        Number of samples to generate\n",
    "    rng: int | np.random.Generator | None (optional)\n",
    "        Random generator or random seed\n",
    "\n",
    "    Example:\n",
    " \n",
    "    >>> means = [[3, 0], [0, 0]]\n",
    "    >>> sigmas = [0.5, 1]\n",
    "    >>> X, Y = mixGauss(means, sigmas, n=100)\n",
    "    >>> fig, ax = plt.subplots()\n",
    "    >>> ax.scatter(X[Y == 1,0], X[Y == 1,1], marker='o', color='r')\n",
    "    >>> ax.scatter(X[Y == -1,0], X[Y == -1,1], marker='o', color='b')\n",
    "    \"\"\"\n",
    "    rng = _check_random_generator(rng)\n",
    "\n",
    "    means = np.asarray(means)\n",
    "    sigmas = np.asarray(sigmas)\n",
    "\n",
    "    num_classes = sigmas.shape[0]\n",
    "    assert means.shape[0] == num_classes, \"Number of `means` and `sigmas` should be the same.\"\n",
    "\n",
    "    d = means.shape[1]\n",
    "    data = np.full((n * num_classes, d), np.inf)\n",
    "    labels = np.zeros(n * num_classes, dtype=np.int64)\n",
    "\n",
    "    for idx, sigma in enumerate(sigmas):\n",
    "        data[idx * n:(idx + 1) * n] = rng.multivariate_normal(\n",
    "            mean=means[idx], cov=np.eye(d) * sigma ** 2, size=n)\n",
    "        labels[idx * n:(idx + 1) * n] = idx\n",
    "\n",
    "    if(num_classes == 2):\n",
    "        labels[labels==0] = -1\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_distances(X1, X2):\n",
    "    return scipy.spatial.distance.cdist(X1, X2, metric='sqeuclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Implementing the LLoyd Algorithm\n",
    "\n",
    "The Lloyd algorithm is the standard algorithm for implementing k-means. It is based on two steps\n",
    " 1. Assigning each point to a cluster\n",
    " 2. Updating the cluster centers\n",
    "repeated iteratively until the cluster centers (and point assignments) have converged.\n",
    "\n",
    "The function skeleton below takes as input the following arguments\n",
    " - `X` the data matrix\n",
    " - `centers` the initial cluster centers. This could be random, or as we will see in the second part, they could be initialized following a smarter strategy.\n",
    " - `maxiter` the maximum number of iterations of the algorithm.\n",
    " \n",
    "The function returns\n",
    " - the final cluster centers\n",
    " - the assignment labels of each point to their cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyd(X, centers, max_iter=1000):\n",
    "    # X: n x d\n",
    "    # centers : k x d\n",
    "    n, d = X.shape\n",
    "    k = centers.shape[0]\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Compute Squared Euclidean distance (i.e. the squared distance)\n",
    "        # between each cluster centre and each observation\n",
    "        # TODO: put your code here\n",
    "        dist = ...\n",
    "\n",
    "        # Assign data to clusters: \n",
    "        # for each point, find the closest center in terms of euclidean distance\n",
    "        # TODO: put your code here\n",
    "        c_asg = ...\n",
    "\n",
    "        # Update cluster center\n",
    "        for c in range(k):\n",
    "            # Remember: cluster may be empty, handle gracefully\n",
    "            # TODO: put your code here\n",
    "            centers[c] = ...\n",
    "\n",
    "    return c_asg, centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. K-Means: Analysis\n",
    "\n",
    "Here you should create a synthetic dataset using the `mixGauss` function **with four or more classes**.\n",
    "\n",
    "You can experiment with different dataset creation strategies:\n",
    " - Create datasets where the classes are very well separated, then k-means should be able to infer the classes easily\n",
    " - Create datasets where the classes have overlap. In this case the k-means algorithm won't be able to distinguish the points of overlap. What do you think will happen?\n",
    " \n",
    "Then, you should run the k-means algorithm with randomly initialized centers:\n",
    " 1. Create the random centers **within the same range as your data**. You can use the `np.random.uniform` function for this.\n",
    " 2. Run the Lloyd algorithm\n",
    " 3. Plot the results.\n",
    "\n",
    "##### Your Tasks\n",
    "You should repeat this procedure multiple times, and comment on the following:\n",
    " 1. Do you obtain the same clusters every time?\n",
    "     If you obtain clusterings which are always the same, you can try to initialize two cluster centers at a very similar point. This will put the algorithm in a difficult situation!\n",
    "     \n",
    " 2. Why do the colors of a specific cluster seem to change at each iteration?\n",
    " 3. Try using the *wrong* number of clusters `k` (wrong with respect to the number of classes you used to generate the data). What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataset with at least 4 classes in 2 dimensions, and plot it\n",
    "# TODO: put your code here\n",
    "X, Y = mixGauss(...)\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=Y, alpha=0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try with a random initialization of the centers.\n",
    "# TODO: put your code here\n",
    "k = ...\n",
    "centers0 = rng.uniform(low=np.amin(X, 0), high=np.amax(X, 0), size=(k, X.shape[1]))\n",
    "\n",
    "# Call the lloyd function\n",
    "# TODO: put your code here\n",
    "Iv, centers = ...\n",
    "\n",
    "# Visualize the final clusters and their centroids\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=Iv, marker='*', alpha=0.8)\n",
    "plt.scatter(centers[:,0], centers[:,1], s=70, c='k', alpha=0.8);\n",
    "\n",
    "# NOTE: Try with different random seeds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. A Better Initialization Strategy: K-Means++\n",
    "\n",
    "Initializing the cluster centers at random, sometimes makes the algorithm converge to a sub-optimal local minimum.\n",
    "\n",
    "The k-means++ algorithm is an **initialization strategy** to generate the initial centers, which can then be passed to the lloyd algorithm.\n",
    "\n",
    "K-means++ works by selecting the cluster centers as the points within the dataset which have the **maximum distance** between each other. To do this it uses a greedy strategy implemented in the `kmeanspp` function below.\n",
    "\n",
    "Take a look at the K-Means++ function and then use it as initialization for the K-means algorithm.\n",
    "\n",
    "##### Your task:\n",
    "You should take a dataset on which K-Means was struggling, and apply K-Means++ for the center initialization, trying to show that the new initialization strategy improves the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeanspp(X, k, rng=None):\n",
    "    rng = _check_random_generator(rng)\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    # initialize centers array\n",
    "    centers = np.empty((k, d), dtype=X.dtype)\n",
    "\n",
    "    # 1. choose first center uniformly at random\n",
    "    first_idx = rng.integers(n)\n",
    "    centers[0] = X[first_idx]\n",
    "\n",
    "    # 2. choose each subsequent center\n",
    "    for i in range(1, k):\n",
    "        # D has shape (i, n): distances from each existing center to each point\n",
    "        D = all_distances(centers[:i], X)  # assume Euclidean distances\n",
    "        # distance to closest center for each point\n",
    "        Ds = np.min(D, axis=0)\n",
    "        D2 = Ds ** 2\n",
    "\n",
    "        # handle degenerate case: all distances zero\n",
    "        total = np.sum(D2)\n",
    "        if total == 0:\n",
    "            # all points are identical to some center: pick uniformly at random\n",
    "            newcpos = rng.integers(n)\n",
    "        else:\n",
    "            P = D2 / total\n",
    "            # sample index according to probabilities P\n",
    "            newcpos = rng.choice(n, p=P)\n",
    "\n",
    "        centers[i] = X[newcpos]\n",
    "\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and use K-Means++ for initialization\n",
    "# TODO: put your code here\n",
    "k = ...\n",
    "centers0 = ...\n",
    "\n",
    "# Call the Lloyd function\n",
    "# TODO: put your code here\n",
    "Iv, centers = ...\n",
    "\n",
    "# Visualize the final clusters and their centroids\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=Iv, alpha=0.8)\n",
    "plt.scatter(centers[:,0], centers[:,1], s=70, c='k', alpha=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the number of clusters affect k-means quality?\n",
    "\n",
    "In this exercise your task is:\n",
    "\n",
    "1. Run k-means clustering with different numbers of clusters (use k-means++ to initialize the centers)\n",
    "2. Evaluate the quality of each clustering using `silhouette_score` from sklearn.\n",
    "3. Plot silhouette score vs. number of clusters and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate datasets where clusters are not perfectly separated (small overlap between clusters)\n",
    "means = np.array([\n",
    "    [0.0, 0.0],\n",
    "    [3.0, 0.0],\n",
    "    [0.0, 3.0],\n",
    "    [-3.0, 0.0],\n",
    "    [0.0, -3.0],\n",
    "])\n",
    "sigmas = np.array([0.9, 0.9, 0.9, 0.9, 0.9])\n",
    "X, Y = mixGauss(means, sigmas, n=200, rng=12)\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=Y, alpha=0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "k_list = list(range(2, 12))\n",
    "sil_scores = []\n",
    "for k in k_list:\n",
    "        # 1) Initialize centers with kmeans++\n",
    "        # TODO: put your code here\n",
    "        centers_init = ...\n",
    "\n",
    "        # 2) Run Lloyd's algorithm starting from these centers\n",
    "        # TODO: put your code here\n",
    "        labels, centers = ...\n",
    "\n",
    "        # 3) Compute silhouette score\n",
    "        # TODO: put your code here, check `silhouette_score` documentation to learn more\n",
    "        score = ...\n",
    "        sil_scores.append(score)\n",
    "\n",
    "# Plot silhouette score vs k\n",
    "plt.figure()\n",
    "plt.plot(k_list, sil_scores, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.title(\"Effect of k on K-Means clustering quality\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
