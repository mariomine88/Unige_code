{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Regularized Least Square\n",
    "In this lab, we focus on RLS to address linear regression problems. \n",
    "\n",
    "In this lab, we have to:\n",
    "- **(Task 1)** implement RLS to solve linear regression problems\n",
    "- **(Task 2)** observe performance of RLS changing the noise in the data and the regularization parameter\n",
    "- **(Task 3)** implement K-Fold Cross-Validation algorithm for RLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "To generate linear regression data, we use the `linearRegrFunction` introduced in Lab0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import Generator\n",
    "\n",
    "\n",
    "def _check_random_generator(rng: int | Generator | None) -> Generator:\n",
    "    \"\"\"Convert rng into a np.random.Generator instance.\"\"\"\n",
    "    if rng is None:\n",
    "        print(\"⚠️  Warning: Set the `rng` parameter to a NumPy random generator\\n\"\n",
    "              \"   (e.g. `np.random.default_rng(42)`) or specify a fixed seed to ensure\\n\"\n",
    "              \"   your results are reproducible. Proceeding with a random seed for now.\")\n",
    "        return np.random.default_rng()\n",
    "    if isinstance(rng, np.random.Generator):\n",
    "        return rng\n",
    "    if isinstance(rng, (int, np.integer)):\n",
    "        return np.random.default_rng(rng)\n",
    "\n",
    "\n",
    "def linearRegrFunction(n, D, low_D, high_D, W, sigma_noise, rng: int | Generator | None = None):\n",
    "    rng = _check_random_generator(rng)\n",
    "\n",
    "    X = np.zeros((n,D), dtype=np.float64)\n",
    "    for i in range(D):\n",
    "        X[:,i] = rng.uniform(low_D[i], high_D[i], size=n)\n",
    "    \n",
    "    gauss_noise = rng.normal(0, sigma_noise, size=(n,1))\n",
    "\n",
    "    Y = np.dot(X, W) + gauss_noise\n",
    "    \n",
    "    return X, Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Noiseless dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "D = 1\n",
    "low_D, high_D = [-1], [1]\n",
    "w = np.array(1.0).reshape(1, 1)\n",
    "noise_std = 0.0\n",
    "random_seed = 42\n",
    "\n",
    "# Data generation\n",
    "X, Y = linearRegrFunction(n, D, low_D, high_D, w, noise_std, rng=random_seed)\n",
    "\n",
    "# Plot of the data\n",
    "_, ax = plt.subplots()\n",
    "ax.set_title(\"Linear Regression data without noise\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: RLS regressor\n",
    "We want to implement the `regularizedLSTrain` function which train RLS regression.\n",
    "\n",
    "The signature of `regularizedLSTrain` is the following:\n",
    "\n",
    "`w = regularizedLSTrain(Xtr, Ytr, lam)`\n",
    "\n",
    "where:\n",
    "- **Xtr** are the training inputs\n",
    "- **Ytr** are the training outputs\n",
    "- **lam** is the regularization parameter $\\lambda$\n",
    "\n",
    "To implement this function, you will need to use the following functions from numpy:\n",
    "\n",
    "- [`np.linalg.cholesky`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cholesky.html)\n",
    "- [`scipy.linalg.solve_triangular`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_triangular.html)\n",
    "\n",
    "Consider \n",
    "\n",
    "**$(X_{tr}^\\intercal X_{tr} + \\lambda n I)w = X_{tr}^\\intercal Y_{tr}$**\n",
    "\n",
    "Let $A = X_{tr}^\\intercal X_{tr} + \\lambda n I$ and $b = X_{tr}^\\intercal Y_{tr}$, we can find $w$ with the following steps:\n",
    "1. First build the left-hand side matrix `A`, and the right-hand side matrix `b`.\n",
    "2. Compute the Cholesky decomposition of `A` (note that the numpy function will provide a lower-triangular matrix)\n",
    "3. You will have to solve two triangular systems, one using the Cholesky decomposition, and the other using its transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizedLSTrain(Xtr, Ytr, lam):\n",
    "    # Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need also to implement a function `regularizedLSTest` which given a test set `Xte` and the `w` obtained using `regularizedLSTrain`, it returns `Ypred` containing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizedLSTest(w, Xte):\n",
    "    # Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of RLS regressor, we need a function to estimate the error.\n",
    "\n",
    "Given two vectors `Ytrue` (real outputs) and `Ypred` (predicted outputs), we can measure the error obtained when predicting `Ypred` instead of `Ytrue` with the MSE (Mean Square Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcError(Ypred, Ytrue):\n",
    "    return np.mean((Ypred-Ytrue)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build an easy example to observe how our model works:\n",
    "- Generate a training set with **ntrain** points and a test set with **ntest** points \n",
    "- Train RLS with `regularizedLSTrain` function and test it with `regularizedLSTest` on test set\n",
    "- Compute the training and test error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 100\n",
    "n_test = 50\n",
    "D = 1\n",
    "low_D = [-1] * D\n",
    "high_D = [5] * D\n",
    "rng = np.random.default_rng(random_seed)\n",
    "w_true = rng.standard_normal((D, 1))\n",
    "noise_std = 0.1\n",
    "\n",
    "lam = 1e-3\n",
    "\n",
    "# Generate a training set with n_train points and a test set with n_test\n",
    "Xtr, Ytr = ...\n",
    "Xte, Yte = ...\n",
    "\n",
    "# Train RLS\n",
    "w_pred = ...\n",
    "\n",
    "# Compute predictions on training and test set\n",
    "Ytr_pred = ...\n",
    "Yte_pred = ...\n",
    "\n",
    "train_err = ...\n",
    "test_err = ...\n",
    "\n",
    "print(f'[--] Training error: {train_err:.7f}\\t Test error: {test_err:.7f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Changing $\\lambda$\n",
    "Now we can play with our model changing the noise level in the data and changing the $\\lambda$ parameter.\n",
    "\n",
    "Let's start by changing $\\lambda$ and fixing the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 500\n",
    "n_test = 100\n",
    "D = 2\n",
    "low_D = [-2] * D\n",
    "high_D = [2] * D\n",
    "w_true = np.arange(D)[:, None] + 10\n",
    "noise_std = 0.01\n",
    "\n",
    "# Data generation\n",
    "Xtr, Ytr = ...\n",
    "Xte, Yte = ...\n",
    "\n",
    "lam_list = np.logspace(-9, 1, 15)\n",
    "tr_err = []\n",
    "te_err = []\n",
    "\n",
    "for lam in lam_list:\n",
    "    # Train RLS\n",
    "    \n",
    "    # Compute predictions on training and test set\n",
    "    \n",
    "    # Compute training and test error and store them on tr_err and te_err\n",
    "\n",
    "\n",
    "# Plot training and test error\n",
    "_, ax = plt.subplots()\n",
    "ax.set_title(\"Training/Test error\")\n",
    "ax.loglog(lam_list, tr_err, '-', c=\"blue\", label=\"training error\")\n",
    "ax.loglog(lam_list, te_err, '-', c=\"orange\", label=\"test error\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain here what happens when $\\lambda$ increases: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: K-Fold Cross Validation for RLS\n",
    "Now, we want to implement the K-Fold Cross Validation for RLS. \n",
    "\n",
    "In specific we want to implement the `KFoldCVRLS` function which, given a training set **Xtr** and **Ytr**, a number of folds **KF** and a set of values for $\\lambda$ (**regpar_list**) and returns the $\\lambda$ which minimize the average validation error **bestlam**, the mean validation error **val_mean**, the validation error variance **val_var**, the mean training error **tr_mean** and the training error variance **tr_var**.\n",
    "\n",
    "`bestlam, val_mean, val_var, tr_mean, tr_var = KFoldCVRLS(Xtr, Ytr, KF, regpar_list)`\n",
    "\n",
    "**Hint:** this function is very similar to K-Fold Cross-Validation algorithm for KNN (Lab1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldCVRLS(Xtr, Ytr, KF, regpar_list, rng: int | Generator | None = None):\n",
    "    assert KF > 1, \"A number of folds (KF) should be > 1\"\n",
    "\n",
    "    rng = _check_random_generator(rng)\n",
    "\n",
    "    # Ensures that regpar_list is a numpy array\n",
    "    regpar_list = np.asarray(regpar_list)\n",
    "    num_regpar = regpar_list.size\n",
    "\n",
    "    n_tot = Xtr.shape[0]\n",
    "    n_val = n_tot // KF\n",
    "\n",
    "    # We want to compute 1 error for each `k` and each fold\n",
    "    tr_errors = np.zeros((num_regpar, KF), dtype=np.float64)\n",
    "    val_errors = np.zeros((num_regpar, KF), dtype=np.float64)\n",
    "\n",
    "    # 1) One permutation, once\n",
    "    rand_idx = rng.permutation(n_tot)\n",
    "    # 2) Split once into K disjoint folds\n",
    "    split_idx = np.array_split(rand_idx, KF)\n",
    "    # 3) Outer loop over folds; inner over hyperparams\n",
    "    for fold in range(KF):\n",
    "        val_mask = np.zeros(n_tot, dtype=bool)\n",
    "        val_mask[split_idx[fold]] = True\n",
    "        train_mask = ~val_mask\n",
    "\n",
    "        for idx, regpar in enumerate(regpar_list):\n",
    "            # Use the boolean mask to split X, Y in training and validation part\n",
    "\n",
    "            X = ... # training input \n",
    "            Y = ... # training output \n",
    "            X_val = ... # validation input\n",
    "            Y_val = ... # validation output\n",
    "            \n",
    "            # Train a RLS model for a single fold, and the given value of `regpar`\n",
    "            currW = ...\n",
    "            \n",
    "            # Compute the training error of the RLS regression for the given value of `regpar`\n",
    "            YpredTR = ...\n",
    "            tr_errors[idx, fold] = calcError(YpredTR, Y)\n",
    "\n",
    "            # Compute the validation error of the RLS regression for the given value of `regpar`\n",
    "            YpredVAL = ...\n",
    "            val_errors[idx, fold] = calcError(YpredVAL, Y_val)\n",
    "\n",
    "    # Calculate error statistics along the repetitions\n",
    "    tr_mean = np.mean(tr_errors, axis=1)\n",
    "    tr_var = np.var(tr_errors, axis=1)\n",
    "    val_mean = np.mean(val_errors, axis=1)\n",
    "    val_var = np.var(val_errors, axis=1)\n",
    "\n",
    "    bestlam_idx = np.argmin(val_mean)\n",
    "    bestlam = regpar_list[bestlam_idx]\n",
    "\n",
    "    return bestlam, val_mean, val_var, tr_mean, tr_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `KFoldCVRLS` to find the best regularization parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1_000\n",
    "D = 1\n",
    "sigma_noise = 0.5\n",
    "rng = np.random.default_rng(random_seed)\n",
    "w_true = rng.standard_normal((D, 1))\n",
    "reg_pars = np.logspace(-5, 1, 100)\n",
    "KF = 5\n",
    "\n",
    "low_D = [-3] * D\n",
    "high_D = [3] * D\n",
    "\n",
    "# Generate training set\n",
    "# Insert your code here\n",
    "Xtr, Ytr = ...\n",
    "Xte, Yte = ...\n",
    "\n",
    "# Compute best lambda\n",
    "bestlam, Vm, Vs, Tm, Ts = ...\n",
    "\n",
    "# Plot training and validation error\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(reg_pars, Vm, '-o', label=\"Validation error\")\n",
    "ax.plot(reg_pars, Tm, '-o', label=\"Train error\")\n",
    "ax.axvline(bestlam, linestyle=\"--\", c=\"red\", alpha=0.7, label=\"best $\\lambda$\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the regression curve using the best $\\lambda$ (found with `KFoldCVRLS`) and using the worst $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[--] Best lambda found: {bestlam:.7f}')\n",
    "# Insert your code here\n",
    "w_best = ...\n",
    "Ypred_best = ...\n",
    "\n",
    "# Insert your code here\n",
    "worstlam = ...\n",
    "print(f'[--] Worst lambda found: {worstlam:.7f}')\n",
    "w_worst = ...\n",
    "Ypred_worst = ...\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.scatter(Xtr, Ytr)\n",
    "idx_te = np.argsort(Xte.squeeze())\n",
    "ax.plot(Xte[idx_te], Ypred_best[idx_te],  c=\"red\", label=\"with best $\\lambda$\")\n",
    "ax.plot(Xte[idx_te], Ypred_worst[idx_te],c=\"black\", label=\"with worst $\\lambda$\")\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best $\\lambda$ found to train the model on the full training set and compute the test error on the following test set.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte, Yte = linearRegrFunction(200, D, low_D, high_D, w_true, sigma_noise, rng=random_seed)\n",
    "print(f'[--] Best lambda found: {bestlam:.7f}')\n",
    "\n",
    "# Insert your code here\n",
    "w_best = ...\n",
    "Ypred_best = regularizedLSTest(w_best, Xte)\n",
    "test_err = ...\n",
    "print(f'[--] Test error: {test_err:.7f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat the procedure on a different dataset\n",
    "\n",
    "Create new training **and** test datasets, sampled in a non-symmetric range (for example you can set the `low_D` and `high_D` parameters of the `linearRegrFunction` function to 2 and 5).\n",
    "\n",
    "Then repeat the K-fold CV procedure, and check whether the best regularization parameter changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
