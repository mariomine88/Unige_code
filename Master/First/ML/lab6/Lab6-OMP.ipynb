{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. 6 - Variable selection\n",
    "\n",
    "In this lab we will move to considering the problems of variable selection in classification (thus supervised) scenarios.\n",
    "\n",
    "As usual, we start importing libraries and functions already used in one of the previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as la\n",
    "\n",
    "rnd_state = np.random.RandomState(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "\n",
    "To generate a synthetic dataset suitable for use with variable selection, we first generate a subset of \"relevant\" features, and then concatenate with a second set of \"dummy\", irrelevant featuers. To this purpose,  we proceed as follows:\n",
    "- Generate a dataset with the `mixGauss` function: use two Gaussians which are *close* with each other (see example below). Start by considering 2-Dimensional points\n",
    "- Plot the points: you should observe two point-clouds which mix with each other (since they were generated with the same parameters).\n",
    "- Enrich the input samples with \"dummy\" features randomly sampled and concatenate to the relevant features (notice that at this point data visualization is no more possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixGauss(means, sigmas, n):\n",
    "\n",
    "    means = np.array(means)\n",
    "    sigmas = np.array(sigmas)\n",
    "\n",
    "    d = means.shape[1]\n",
    "    num_classes = sigmas.size\n",
    "    data = np.full((n * num_classes, d), np.inf)\n",
    "    labels = np.zeros(n * num_classes)\n",
    "\n",
    "    for idx, sigma in enumerate(sigmas):\n",
    "        data[idx * n:(idx + 1) * n] = rnd_state.multivariate_normal(\n",
    "            mean=means[idx], cov=np.eye(d) * sigmas[idx] ** 2, size=n)\n",
    "        labels[idx * n:(idx + 1) * n] = idx \n",
    "        \n",
    "    if(num_classes == 2):\n",
    "        labels[labels==0] = -1\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000     # number of features for each gaussian\n",
    "d=30       # total number of features\n",
    "d_rev = 2  # number of relevant features\n",
    "\n",
    "Xtr, Ytr = mixGauss(means = [[0,0],[2,2]], sigmas = [0.9, 0.75], n=n)\n",
    "Xte, Yte = mixGauss(means = [[0,0],[2,2]], sigmas = [0.9, 0.75], n=n)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Training Set\", fontsize=22)\n",
    "ax.scatter(Xtr[Ytr==-1,0], Xtr[Ytr==-1,1], s=30, alpha=0.80)\n",
    "ax.scatter(Xtr[Ytr==1,0], Xtr[Ytr==1,1], s=30, alpha=0.80)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Test Set\", fontsize=22)\n",
    "ax.scatter(Xte[Yte==-1,0], Xte[Yte==-1,1], s=30, alpha=0.80)\n",
    "ax.scatter(Xte[Yte==1,0], Xte[Yte==1,1], s=30, alpha=0.80);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy features generation\n",
    "sigma_noise = 0.13\n",
    "Xtr_noise = sigma_noise * rnd_state.randn(2*n, d-d_rev)\n",
    "Xtr = np.concatenate((Xtr,Xtr_noise), axis=1)\n",
    "Xte_noise = sigma_noise * rnd_state.randn(2*n, d-d_rev)\n",
    "Xte = np.concatenate((Xte,Xte_noise), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OMatchingPursuit\n",
    "\n",
    "It computes a sparse representation of the signal using Orthogonal Matching Pursuit algorithm. Use it as follows:\n",
    "    \n",
    "`w, r, I = OMatchingPursuit( X, Y, T)`\n",
    "\n",
    "where\n",
    "    - X: input data\n",
    "    - Y: output labels\n",
    "    - T: number of iterations\n",
    "    - w: estimated coefficients\n",
    "    - r: residuals\n",
    "    - I: indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OMatchingPursuit(X, Y, T):\n",
    "\n",
    "    N, D = np.shape(X)\n",
    "\n",
    "    # 1. Initialization of residual, coefficient vector and index set I\n",
    "    r = Y.copy()\n",
    "    w = np.zeros(D)\n",
    "    I = []\n",
    "\n",
    "    for i in range(T):\n",
    "        # 2. Select the column of X which coefficients most \"explains\" the residual\n",
    "        a_max = -np.inf\n",
    "        j_max = None\n",
    "\n",
    "        for j in range(D):\n",
    "            # correlation between column j and residual\n",
    "            a_tmp = ...\n",
    "\n",
    "            if a_tmp > a_max:\n",
    "                a_max = a_tmp\n",
    "                j_max = j\n",
    "\n",
    "        # 3. Add the index to the set of indexes\n",
    "        assert j_max is not None, \"OMP failed to select a coordinate â€” check correlation computation.\"\n",
    "        if j_max not in I:\n",
    "            I.append(j_max)\n",
    "\n",
    "        # Compute the M matrix\n",
    "        M_I = np.zeros((D, D))\n",
    "        for j in I:\n",
    "            M_I[j, j] = 1\n",
    "\n",
    "        A = M_I.dot(X.T).dot(X).dot(M_I)\n",
    "        B = M_I.dot(X.T).dot(Y)\n",
    "\n",
    "        # 4. Update estimated coefficients\n",
    "        w = ...\n",
    "\n",
    "        # 5. Update the residual\n",
    "        r = ...\n",
    "\n",
    "    return w, r, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual function to compute the error in predicted labels (for a binary classification problem)\n",
    "def calcError(Ypred, Y):\n",
    "    V = np.multiply(np.sign(Ypred), np.sign(Y))\n",
    "    return np.count_nonzero(V < 0) / len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation for selecting the best value for the hyperparameters in the use of \n",
    "# Orthogonal Matching Pursuit (we refer to the number of iterations)\n",
    "\n",
    "def KFoldCVOMP(Xtr, Ytr, KF, niter_list):\n",
    "    \"\"\"\n",
    "    K-Fold Cross Validation to select the best number of iterations for OMP.\n",
    "\n",
    "    Returns:\n",
    "        best_niter, Vm, Vs, Tm, Ts\n",
    "    \"\"\"\n",
    "    assert KF > 1, \"KF must be an integer >= 2\"\n",
    "    n_tot = Xtr.shape[0]\n",
    "    assert KF <= n_tot, \"KF cannot be larger than the number of samples\"\n",
    "\n",
    "    # Ensure numpy array\n",
    "    niter_list = np.array(niter_list, dtype=int)\n",
    "    num_niter = niter_list.size\n",
    "\n",
    "    # Random permutation of training data\n",
    "    rand_idx = rnd_state.permutation(n_tot)\n",
    "\n",
    "    # Size of each validation fold (may leave a remainder)\n",
    "    n_val = n_tot // KF  # integer division\n",
    "\n",
    "    # Accumulators: sums over folds\n",
    "    Tm = np.zeros(num_niter, dtype=float)  # mean training error accumulator\n",
    "    Ts = np.zeros(num_niter, dtype=float)  # second moment of training error\n",
    "    Vm = np.zeros(num_niter, dtype=float)  # mean validation error accumulator\n",
    "    Vs = np.zeros(num_niter, dtype=float)  # second moment of validation error\n",
    "\n",
    "    for kdx, niter in enumerate(niter_list):\n",
    "        # Reset fold pointer per hyperparameter so folds are identical for each niter\n",
    "        first = 0\n",
    "\n",
    "        for fold in range(KF):\n",
    "            start = fold * n_val\n",
    "            end = (fold + 1) * n_val if fold < KF - 1 else n_tot  # last fold uses all remaining\n",
    "\n",
    "            val_idx = rand_idx[start:end]\n",
    "            train_idx = np.setdiff1d(rand_idx, val_idx, assume_unique=True)\n",
    "\n",
    "            X = Xtr[train_idx]\n",
    "            Y = Ytr[train_idx]\n",
    "            X_val = Xtr[val_idx]\n",
    "            Y_val = Ytr[val_idx]\n",
    "\n",
    "            # Train OMP\n",
    "            w, r, I = OMatchingPursuit(X, Y, niter)\n",
    "\n",
    "            # Training error\n",
    "            YpredTR = np.sign(X.dot(w))\n",
    "            trError = calcError(YpredTR, Y)\n",
    "            Tm[kdx] += trError\n",
    "            Ts[kdx] += trError ** 2\n",
    "\n",
    "            # Validation error\n",
    "            YpredVAL = np.sign(X_val.dot(w))\n",
    "            valError = calcError(YpredVAL, Y_val)\n",
    "            Vm[kdx] += valError\n",
    "            Vs[kdx] += valError ** 2\n",
    "\n",
    "    # Average over folds\n",
    "    Tm /= KF\n",
    "    Ts = Ts / KF - Tm**2  # variance estimate\n",
    "\n",
    "    Vm /= KF\n",
    "    Vs = Vs / KF - Vm**2  # variance estimate\n",
    "\n",
    "    # Sometimes tiny negative variances appear due to numerical issues\n",
    "    Ts = np.maximum(Ts, 0.0)\n",
    "    Vs = np.maximum(Vs, 0.0)\n",
    "\n",
    "    best_niter_idx = np.argmin(Vm)\n",
    "    best_niter = niter_list[best_niter_idx]\n",
    "\n",
    "    print(\"Validation mean errors:\", Vm)\n",
    "    print(\"niter_list:\", niter_list)\n",
    "    print(\"Best index:\", best_niter_idx)\n",
    "\n",
    "    return best_niter, Vm, Vs, Tm, Ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some analysis\n",
    "\n",
    "We suggest to proceed as follows:\n",
    "\n",
    "- Standardize the data\n",
    "- Run Orthogonal Matching Pursuit on the training set setting a reasonable number of iterations\n",
    "- Compute the prediction on the test set and evaluate the error\n",
    "- Plot the components of the solution w (considering their absolute value): how do they look?\n",
    "- Run the K-Fold Cross Validation to select an appropriate value for the number of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data standardization\n",
    "m = np.mean(Xtr, axis=0)\n",
    "s = np.std(Xtr, axis=0)\n",
    "Xtrnorm = np.divide((Xtr - m), s)\n",
    "Xtenorm = np.divide((Xte - m), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Orthogonal Matching Pursuit\n",
    "# ... fill here ...\n",
    "\n",
    "# Compute the prediction on the test set\n",
    "# ... fill here ...\n",
    "\n",
    "# Compute the test error and show its value\n",
    "# ... fill here ...\n",
    "\n",
    "# Plot the components of the solution w\n",
    "# ... fill here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run K-Fold Cross Validation\n",
    "KF = 5\n",
    "niter_list = [1, 2, 3, 4, 5, 10, d] \n",
    "best_niter, Vm, Vs, Tm, Ts = ...\n",
    "\n",
    "# Plot training and validation error\n",
    "plt.subplots()\n",
    "plt.plot(niter_list, Tm, label=\"training\")\n",
    "plt.plot(niter_list, Vm, label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.axvline(best_niter, color=\"red\")\n",
    "print(f'Best number of iterations: {best_niter}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also shuffle columns of the synthetic dataset in order to observe the behavior of OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle columns\n",
    "shuffle_idx = rnd_state.choice(Xtr.shape[1], Xtr.shape[1], replace=False)\n",
    "\n",
    "Xtr = Xtr[:, shuffle_idx]\n",
    "Xte = Xte[:, shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[--] The relevant features are {(shuffle_idx == 0).argmax()} and {(shuffle_idx == 1).argmax()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OMP and verify that it is able to find the relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the number of relevant and dummy features and observe the behavior of OMP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
